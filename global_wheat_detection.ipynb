{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# training dataset\n",
    "DATA_DIR = 'wheat'\n",
    "\n",
    "anns = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "anns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "test_dir = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_dicom_fps(dicom_dir):\n",
    "    dicom_fps = glob.glob(dicom_dir+'/'+'*.jpg')\n",
    "    return list(set(dicom_fps))\n",
    "\n",
    "def parse_dataset(dicom_dir, anns): \n",
    "    image_fps = get_dicom_fps(dicom_dir)\n",
    "    image_annotations = {fp: [] for fp in image_fps}\n",
    "    for index, row in anns.iterrows(): \n",
    "        fp = os.path.join(dicom_dir, row['image_id'] +'.jpg')\n",
    "        image_annotations[fp].append(row)\n",
    "    return image_fps, image_annotations \n",
    "\n",
    "image_fps, image_annotations = parse_dataset(train_dir, anns=anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a mask rcnn on the kangaroo dataset\n",
    "from os import listdir\n",
    "from xml.etree import ElementTree\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from mrcnn.utils import Dataset\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "\n",
    "\n",
    "\n",
    "# class that defines and loads the wheat dataset\n",
    "class WheatDataset(Dataset):\n",
    "    # load the dataset definitions\n",
    "    \n",
    "    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        # Add classes\n",
    "        self.add_class('dataset', 1, 'wheat')\n",
    "        \n",
    "        # add images \n",
    "        for i, fp in enumerate(image_fps):\n",
    "            annotations = image_annotations[fp]\n",
    "            self.add_image('dataset', image_id=i, path=fp, \n",
    "                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        fp = info['path']\n",
    "        ds = Image.open(fp)\n",
    "\n",
    "        image = np.asarray(ds)\n",
    "#         ds = pydicom.dcmread(fp)\n",
    "\n",
    "#         image = ds.pixel_array\n",
    "        # If grayscale. Convert to RGB for consistency.\n",
    "        if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "            image = np.stack((image,) * 3, -1)\n",
    "        return image\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        annotations = info['annotations']\n",
    "        count = len(annotations)\n",
    "        if count == 0:\n",
    "            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n",
    "            class_ids = np.zeros((1,), dtype=np.int32)\n",
    "        else:\n",
    "            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n",
    "            class_ids = np.zeros((count,), dtype=np.int32)\n",
    "            for i, a in enumerate(annotations):\n",
    "                box = eval(a['bbox'])\n",
    "                #print(box)\n",
    "                row_s, row_e = int(box[1]), int(box[3])\n",
    "                col_s, col_e = int(box[0]), int(box[2])\n",
    "                #mask_instance = mask[:, :, i].copy()\n",
    "                #mask[row_s:row_e, col_s:col_e, i] = 1\n",
    "                mask_instance = mask[:, :, i].copy()\n",
    "                cv2.rectangle(mask_instance, (row_s, col_s), (row_e, col_e), 255, -1)\n",
    "                mask[:, :, i] = mask_instance\n",
    "                class_ids[i] = 1\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "ORIG_SIZE = 1024\n",
    "\n",
    "# split dataset into training vs. validation dataset \n",
    "image_fps_list = list(image_fps)\n",
    "random.seed(42)\n",
    "random.shuffle(image_fps_list)\n",
    "\n",
    "val_size = 1500\n",
    "image_fps_val = image_fps_list[:val_size]\n",
    "image_fps_train = image_fps_list[val_size:]\n",
    "\n",
    "print(len(image_fps_train), len(image_fps_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the training dataset\n",
    "dataset_train = WheatDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\n",
    "dataset_train.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show annotation(s) for a DICOM image \n",
    "test_fp = random.choice(image_fps_train)\n",
    "image_annotations[test_fp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the validation dataset\n",
    "dataset_val = WheatDataset(image_fps_val, image_annotations, ORIG_SIZE, ORIG_SIZE)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pydicom\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Mask RCNN\n",
    "sys.path.append(os.path.join('./', 'Mask_RCNN'))  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following parameters have been selected to reduce running time for demonstration purposes \n",
    "# These are not optimal \n",
    "\n",
    "class WheatConfig(Config):\n",
    "    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n",
    "    Overrides values in the base Config class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Give the configuration a recognizable name  \n",
    "    NAME = 'wheat'\n",
    "    \n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "    \n",
    "    BACKBONE = 'resnet50'\n",
    "    \n",
    "    NUM_CLASSES = 2  # background + 1 pneumonia classes\n",
    "    \n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "    #RPN_ANCHOR_SCALES = (32, 64, 128, 256)\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "    MAX_GT_INSTANCES = 3\n",
    "    DETECTION_MAX_INSTANCES = 3\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\n",
    "    DETECTION_NMS_THRESHOLD = 0.1\n",
    "\n",
    "    STEPS_PER_EPOCH = 2\n",
    "    \n",
    "config = WheatConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display random sample and their bounding boxes\n",
    "\n",
    "class_ids = [0]\n",
    "while class_ids[0] == 0:  ## look for a mask\n",
    "    image_id = random.choice(dataset_train.image_ids)\n",
    "    image_fp = dataset_train.image_reference(image_id)\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "masked = np.zeros(image.shape[:2])\n",
    "for i in range(mask.shape[2]):\n",
    "    masked += image[:, :, 0] * mask[:, :, i]\n",
    "plt.imshow(masked, cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image augmentation (light but constant)\n",
    "augmentation = iaa.Sequential([\n",
    "    iaa.OneOf([ ## geometric transform\n",
    "        iaa.Affine(\n",
    "            scale={\"x\": (0.98, 1.04), \"y\": (0.98, 1.04)},\n",
    "            translate_percent={\"x\": (-0.03, 0.03), \"y\": (-0.05, 0.05)},\n",
    "            rotate=(-5, 5),\n",
    "            shear=(-3, 3),\n",
    "        ),\n",
    "        iaa.PiecewiseAffine(scale=(0.002, 0.03)),\n",
    "    ]),\n",
    "    iaa.OneOf([ ## brightness or contrast\n",
    "        iaa.Multiply((0.85, 1.15)),\n",
    "        iaa.ContrastNormalization((0.85, 1.15)),\n",
    "    ]),\n",
    "    iaa.OneOf([ ## blur or sharpen\n",
    "        iaa.GaussianBlur(sigma=(0.0, 0.12)),\n",
    "        iaa.Sharpen(alpha=(0.0, 0.12)),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "# test on the same image as above\n",
    "imggrid = augmentation.draw_grid(image[:, :, 0], cols=5, rows=2)\n",
    "plt.figure(figsize=(30, 12))\n",
    "_ = plt.imshow(imggrid[:, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modellib.MaskRCNN(mode='training', config=config, model_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Train Mask-RCNN Model \n",
    "# import warnings \n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## first epochs with higher lr to speedup the learning\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            epochs=2,\n",
    "            layers='all',\n",
    "           augmentation=None)  ## no need to augment yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            layers='all',\n",
    "            augmentation=augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select trained model \n",
    "dir_names = next(os.walk(model.model_dir))[1]\n",
    "key = config.NAME.lower()\n",
    "dir_names = filter(lambda f: f.startswith(key), dir_names)\n",
    "dir_names = sorted(dir_names)\n",
    "\n",
    "if not dir_names:\n",
    "    import errno\n",
    "    raise FileNotFoundError(\n",
    "        errno.ENOENT,\n",
    "        \"Could not find model directory under {}\".format(self.model_dir))\n",
    "    \n",
    "fps = []\n",
    "# Pick last directory\n",
    "for d in dir_names: \n",
    "    dir_name = os.path.join(model.model_dir, d)\n",
    "    # Find the last checkpoint\n",
    "    checkpoints = next(os.walk(dir_name))[2]\n",
    "    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n",
    "    checkpoints = sorted(checkpoints)\n",
    "    if not checkpoints:\n",
    "        print('No weight files in {}'.format(dir_name))\n",
    "    else:\n",
    "        checkpoint = os.path.join(dir_name, checkpoints[-1])\n",
    "        fps.append(checkpoint)\n",
    "\n",
    "model_path = sorted(fps)[-1]\n",
    "print('Found model {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(WheatConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode='inference', \n",
    "                          config=inference_config,\n",
    "                          model_dir='./')\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color for class\n",
    "def get_colors_for_class_ids(class_ids):\n",
    "    colors = []\n",
    "    for class_id in class_ids:\n",
    "        if class_id == 1:\n",
    "            colors.append((.941, .204, .204))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show few example of ground truth vs. predictions on the validation dataset \n",
    "dataset = dataset_val\n",
    "fig = plt.figure(figsize=(10, 30))\n",
    "\n",
    "for i in range(6):\n",
    "    image_id = random.choice(dataset.image_ids)\n",
    "    \n",
    "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask = \\\n",
    "        modellib.load_image_gt(dataset_val, inference_config, \n",
    "                               image_id, use_mini_mask=False)\n",
    "    \n",
    "    print(original_image.shape)\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                                dataset.class_names,\n",
    "                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n",
    "    \n",
    "    plt.subplot(6, 2, 2*i + 2)\n",
    "    results = model.detect([original_image]) #, verbose=1)\n",
    "    r = results[0]\n",
    "    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                dataset.class_names, r['scores'], \n",
    "                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames of test dataset DICOM images\n",
    "test_image_fps = get_dicom_fps(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test images, write out sample submission\n",
    "def predict(image_fps, filepath='submission.csv', min_conf=0.98):\n",
    "    # assume square image\n",
    "    resize_factor = ORIG_SIZE / config.IMAGE_SHAPE[0]\n",
    "    #resize_factor = ORIG_SIZE\n",
    "    with open(filepath, 'w') as file:\n",
    "        file.write(\"image_id,PredictionString\\n\")\n",
    "\n",
    "        for image_id in tqdm(image_fps):\n",
    "            ds = Image.open(image_id)\n",
    "            image = np.asarray(ds)\n",
    "            #image = ds.pixel_array\n",
    "            # If grayscale. Convert to RGB for consistency.\n",
    "            if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "                image = np.stack((image,) * 3, -1)\n",
    "            image, window, scale, padding, crop = utils.resize_image(\n",
    "                image,\n",
    "                min_dim=config.IMAGE_MIN_DIM,\n",
    "                min_scale=config.IMAGE_MIN_SCALE,\n",
    "                max_dim=config.IMAGE_MAX_DIM,\n",
    "                mode=config.IMAGE_RESIZE_MODE)\n",
    "\n",
    "            img_id = os.path.splitext(os.path.basename(image_id))[0]\n",
    "\n",
    "            results = model.detect([image])\n",
    "            r = results[0]\n",
    "\n",
    "            out_str = \"\"\n",
    "            out_str += img_id\n",
    "            out_str += \",\"\n",
    "            assert( len(r['rois']) == len(r['class_ids']) == len(r['scores']) )\n",
    "            if len(r['rois']) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                num_instances = len(r['rois'])\n",
    "\n",
    "                for i in range(num_instances):\n",
    "                    if r['scores'][i] > min_conf:\n",
    "                        out_str += ' '\n",
    "                        out_str += str(round(r['scores'][i], 2))\n",
    "                        out_str += ' '\n",
    "\n",
    "                        # x1, y1, width, height\n",
    "                        x1 = r['rois'][i][1]\n",
    "                        y1 = r['rois'][i][0]\n",
    "                        width = r['rois'][i][3] - x1\n",
    "                        height = r['rois'][i][2] - y1\n",
    "                        bboxes_str = \"{} {} {} {}\".format(x1*resize_factor, y1*resize_factor, \\\n",
    "                                                           width*resize_factor, height*resize_factor)\n",
    "                        out_str += bboxes_str\n",
    "\n",
    "            file.write(out_str+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_fp = os.path.join('./', 'submission.csv')\n",
    "predict(test_image_fps, filepath=submission_fp)\n",
    "print(submission_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv(submission_fp, header=0)\n",
    "output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few test image detection example\n",
    "def visualize(): \n",
    "    image_id = random.choice(test_image_fps)\n",
    "    ds = pydicom.read_file(image_id)\n",
    "    \n",
    "    # original image \n",
    "    image = ds.pixel_array\n",
    "    \n",
    "    # assume square image \n",
    "    resize_factor = ORIG_SIZE / config.IMAGE_SHAPE[0]\n",
    "    \n",
    "    # If grayscale. Convert to RGB for consistency.\n",
    "    if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "        image = np.stack((image,) * 3, -1) \n",
    "    resized_image, window, scale, padding, crop = utils.resize_image(\n",
    "        image,\n",
    "        min_dim=config.IMAGE_MIN_DIM,\n",
    "        min_scale=config.IMAGE_MIN_SCALE,\n",
    "        max_dim=config.IMAGE_MAX_DIM,\n",
    "        mode=config.IMAGE_RESIZE_MODE)\n",
    "\n",
    "    patient_id = os.path.splitext(os.path.basename(image_id))[0]\n",
    "    print(patient_id)\n",
    "\n",
    "    results = model.detect([resized_image])\n",
    "    r = results[0]\n",
    "    for bbox in r['rois']: \n",
    "        print(bbox)\n",
    "        x1 = int(bbox[1] * resize_factor)\n",
    "        y1 = int(bbox[0] * resize_factor)\n",
    "        x2 = int(bbox[3] * resize_factor)\n",
    "        y2 = int(bbox[2]  * resize_factor)\n",
    "        cv2.rectangle(image, (x1,y1), (x2,y2), (77, 255, 9), 3, 1)\n",
    "        width = x2 - x1 \n",
    "        height = y2 - y1 \n",
    "        print(\"x {} y {} h {} w {}\".format(x1, y1, width, height))\n",
    "    plt.figure() \n",
    "    plt.imshow(image, cmap=plt.cm.gist_gray)\n",
    "\n",
    "visualize()\n",
    "visualize()\n",
    "visualize()\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
